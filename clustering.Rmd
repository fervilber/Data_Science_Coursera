---
title: "Clustering"
author: "Fernando Villalba"
date: "13 de febrero de 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Agrupamiento de datos

Un algoritmo de agrupamiento (en inglés, **clustering**) es un procedimiento de agrupación de una serie de vectores de acuerdo con un criterio. Esos criterios son por lo general distancia o similitud. La cercanía se define en términos de una determinada función de distancia, como la euclídea, aunque existen otras más robustas o que permiten extenderla a variables discretas. La medida más utilizada para medir la similitud entre los casos es la matriz de correlación entre los n x n casos. Sin embargo, también existen muchos algoritmos que se basan en la máximización de una propiedad estadística llamada verosimilitud.

Generalmente, los vectores de un mismo grupo (o clústers) comparten propiedades comunes.El conocimiento de los grupos puede permitir una descripción sintética de un conjunto de datos multidimensional complejo. De ahí su uso en minería de datos. Esta descripción sintética se consigue sustituyendo la descripción de todos los elementos de un grupo por la de un representante característico del mismo.



# Agrupamiento jerárquico (Hierarchical Clustering)

En minería de datos, el agrupamiento jerárquico (**Hierarchical Clustering**) es un método de análisis que busca construir una **jerarquía** de grupos de forma automatizada a partir de lo juntos que estén los datos según funciones de distancia.

Es util para ver datos de alta dimensionalidad, consiste en agrupar los datos que están juntos en grupos.

* **agglomerative approach** (most common) â€” bottom up
	1. start with data
	2. find closest pairs, put them together (create "super point" and remove original data)
	3. find the next closest
	4. repeat = yields a tree showing order of merging (dendrogram)
	* requires
		* ***merging approach***: how to merge two points
		* ***distance metric***: calculating distance between two points
		* **continuous** - *Euclidean distance* $\rightarrow$ $\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + \dots + (Z_1 -Z_2)^2 }$
		* **continuous** - *correlation similarity* $\rightarrow$ how correlated two data points are
		* **binary** - *Manhattan distance* ("city block distance") $\rightarrow$ $|A_1 - A_2| + |B_1 - B_2| + \dots + |Z_1 -Z_2|$

```{r fig.height = 3, fig.width = 6, fig.align='center', echo=FALSE}
#install.packages("png")
library(png)
library(grid)
setwd("C:/R/proyectos/Data_Science_Coursera/04_EXPLORAR_Y_ANALIZAR_DATOS/")
grid.raster(readPNG("figures/25.png"))
```

## Procedimiento para construir un Hierarchical Clusters (funcion `hclust`)
El proceso es simple:

1. calculamos la distancia entre cada par de puntos para ver  cual está más cerca
	* `dist(data.frame(x=x, y=y)` = returns pair wise distances for all of the (x,y) coordinates
	* ***Note**: `dist()` function uses Euclidean distance by default *
2. Agrupamos los dos puntos más cercanos en base a la tabla de dsitancia anterior y los sustituimos por un único punto agrupado.
3. Volvemos al paso anterior buscando los siguientes 2 puntos más cercanos y los juntamos .. y así hasta juntar todos los puntos en uno
4. El orden del agrupamiento se muestra en un dendrograma

Un **dendrograma** es un tipo de representación gráfica o diagrama de datos en forma de árbol que organiza los datos en subcategorías que se van dividiendo en otros hasta llegar al nivel de detalle deseado.

Observando las sucesivas subdivisiones podemos hacernos una idea sobre los criterios de agrupación de los mismos, la distancia entre los datos según las relaciones establecidas, etc.
Hay que tener presenta varias cosas, primero que las agrupaciones son gráficos inestables en el sentido que pequeños cambios en los puntos pueden producir grandes cambios en la froma de agruparse. 

También que el uso de diferentes métricas (funciones de distancia) conlleva diferentes resultados.
Por otro lado las salidas de la funcion `hclust` son deterministas, en el sentido que los mismos argumentos de entrada, producen siempre la misma salida o resultado.
Determinar cuantos cluster o agrupamientos hay no siempre es sencillo, pero son graficos apropiados para un analisis exploatorio de datos, que pueden permitirnos comprobar de un vistazo si existe o no patrones de comportamiento en los datos.

### `hclust` Función 

La funcion `hclust` puede utilizar dos argumento para `method = "complete"` o `"method = average"`. Podemos traducirlos como vinculación completa o promedio.

* ***average linkage*** = toma la distancia media ... centro de masas de los puntos del cluster
* ***complete linkage*** = toma la distancia más alejada entre los puntos posible
* ***Note**: cada aproximación prodice resultados diferentes*


* `hh <- hclust(dist(dataFrame))` función = produce un **objeto** agrupamiento jerarquico de los datos basado en pares de distancias entre los datos de un data frame con valores x e y.
	* `dist()` = por defecto es distancia Euclidea, calcula la distancia o similitud entre cada 2 observaciones de los datos; cuando se aplica sobre un data frame, la funcion aplica esta formula a cada par de datos: $\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + ... + (Z_1 -Z_2)^2 }$ .Construye como resultado una matriz de distancias entre puntos (por fila- observaciones).
		* El orden del agrupamiento se determina por estas distancias
	* `plot(hh)` = pinbta el dendrograma
	* Ordena automáticamente las columnas y filas
	* `names(hh)` = devuelve todos los nombres del objeto `hclust`
		* `hh$order` = devuelve el orden de las filas/clusters del dendrograma
		* `hh$dist.method` = devuelve el metodo usado para el calculo de la distancia o similitud entre puntos.
* ***Note**: El **dendrograma no dice cuantos cluster hay** , para saber esto hay que hacer un corte en un nivel del mismo.

### `hclust` Ejemplos 
El proceso es: 

1. calcular la distancia.
2. calcular el dendrograma.

```{r fig.height = 3, fig.width = 4, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
#pintamos los puntos
plot(x,y,col="blue",pch=19)
text(x = x, y = y, labels = rownames(dataFrame), pos=4, col="red")

#calculamos la distancia
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
#hClustering <- hclust(distxy, method = "complete")
#hClustering <- hclust(distxy, method = "average")
plot(hClustering)
```


### `myplcclust` Función

La función `myplcclust`, cuyo código se muestra a continuación, es una variante de `hclust`, en la que se pinta y etiqueta cada cluster con un número y un color. 
Hay que saber como argumento el número de clustres inicial, pr lo que debemos antes ejecutar `hclust`

```{r}
myplclust <- function(hclust, lab = hclust$labels,
	lab.col = rep(1, length(hclust$labels)), hang = 0.1, ...) {
	## modifiction of plclust for plotting hclust objects *in colour*! Copyright
	## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector
	## of labels of the leaves of the tree lab.col: colour for the labels;
	## NA=default device foreground colour hang: as in hclust & plclust Side
	## effect: A display of hierarchical cluster with coloured leaf labels.
	y <- rep(hclust$height, 2)
	x <- as.numeric(hclust$merge)
	y <- y[which(x < 0)]
	x <- x[which(x < 0)]
	x <- abs(x)
	y <- y[order(x)]
	x <- x[order(x)]
	plot(hclust, labels = FALSE, hang = hang, ...)
	text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
 col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)
}
```

Veamos un ejemplo:

```{r fig.height = 3, fig.width = 4, fig.align='center'}
# example
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

### `heatmap` Función
Otra funcion interesante para representar el agrupamiento es `heatmap(data.matrix)`, que es una función similar a `image(t(x))`. Es una buena forma de representar gráficos multidimensionales, de muchas dimensiones que toman de base una matriz multidimensional.
El argumento debe ser una matriz, por lo que es util `as.matrix(data.frame)` para convertir en caso necesario.

Lo que hace es realizar el agrupamiento jerarquico no solo por observaciones o filas como `hclust`, sino tambien en las columnas o variables, por lo que es como tomar las variables como subconjuntos de observaciones y vemos en una simple imagen qué variables son más cercanas o están más relacionadas que otras.
Esto tiene sentido con matrices de muchas variables, no para una de solo dos x,y, pues no dice la relacion entre columnas.

EL mapa final se muestra en dos colores, el amarillo indican relación alta y el rojo relación a la baja o contraria

```{r fig.height = 3, fig.width = 4, fig.align='center'}
set.seed(12345)
data <- matrix(rnorm(400), nrow = 40)
# data tiene 40 filas y 10 col o variables
heatmap(data)

```

```{r}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
y1<- y*2
y2<-y1+.5
dataFrame <- data.frame(x,y,y1,y2)
heatmap(as.matrix(dataFrame))
```

### `image` Función
La función `image()` produce una salida identica a `heatmap()`,pero sin los dendrogramas.
* `image(x, y, t(dataMatrix)[, nrow(dataMatrix):1])` 
	* `t(dataMatrix)[, nrow(dataMatrix)]`
		* `t(dataMatrix)` = transpose of dataMatrix, this is such that the plot will be displayed in the same fashion as the matrix (rows as values on the y axis and columns as values on the x axis)
			* ***example*** 40 x 10 matrix will have graph the 10 columns as x values and 40 rows as y values
		* `[, nrow(dataMatrix)]` = subsets the data frame in reverse column order; when combined with the `t()` function, it reorders the rows of data from 40 to 1, such that the data from the matrix is displayed in order from top to bottom
			* ***Note**: without this statement the rows will be displayed in order from bottom to top, as that is in line with the positive y axis *
	* `x`, `y` = used to specify the values displayed on the x and y axis
		* ***Note**: must be in increasing order *
* ***example***

```{r fig.height = 4, fig.width = 3, fig.align='center'}
set.seed(12345)
data <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data)[, nrow(data):1])
```

$\pagebreak$

## K-means Clustering

Otra técnica de agrupamiento, muy antigua, es el llamado k-mean clustering, que en el fondo es similar al agrupamiento jerarquico anterior, que proporciona una manera bastante eficiente de resumir datos multidimensionales y ver si existen patrones entre ellos, si hay similitud en las observaciones etc.

K-means es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Es un método utilizado en minería de datos.

Lo principal del agrupamiento es la definición de qué significa cercanía, qué es lo que está cerca y qué lejos.
La forma de calcular esta distancia es la clave. Como sabemos la manera más intuitiva es definir la distancia física entre dos puntos, pero podemos considerar tambien la cercanía o no según la correlación de dos variables..otra opción es considerar la distancia Manhattan, para reticulas..

El algoritmo K-mean parte del conocimiento del número de grupos inicial que hay, y de unas coordenadas inciales para cada centroide de cada grupo.

Iterativamente va a asignar los puntos a un grupo por cercanía al centroide, y recalcula el centroide con los puntos de agrupados...y así hasta que localiza el centro de cada agrupación.  

Este algoritmo **no es deterministico**, pues parte de una agrupacion incial estimada y puede ofrecer diferentes resultados para los mismos parámetros iniciales.


###  Funcion (`kmeans`)
* **Proceso de cálculo del algoritmo K-Means**
	1. Establece el numero inicial de grupos (o clusters)
	2. Busca el centroide de cada uno
	3. asigna los puntos al centroide más cercano.
	4. recalcula el centroide
	5. repite = vuelve alpaso 2.. y así
	* **requiere**
		* definir la distancia
		* numero inicial de clusters
		* sugerir la posición inicial de cada centroide para cada agrupación

* ***Ejemplo***

```{r fig.height = 2, fig.width = 3, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
# specifies initial number of clusters to be 3
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj) # nos da los argumentos posibles de kmeans
# returns cluster assignments
kmeansObj$cluster #calcula los clusters
#para pintar los resultados
par(mar=rep(0.2,4))
#pintamos los puntos primero
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
#despues pintamos los centroides de los clusters
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```

**Ejemplo 2**
```{r fig.height = 2, fig.width = 3, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2<-kmeans(dataMatrix, centers = 3)

par(mfrow=c(1,2), mar=c(2,4,0.1,0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1], yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)], yaxt="n")


dataFrame <- data.frame(x=x,y=y)
# specifies initial number of clusters to be 3
kmeansObj <- kmeans(dataFrame,centers=3)
```


$\pagebreak$

# Análisis de componentes principales

En estadística, el análisis de componentes principales (en español ACP, en inglés, PCA) es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos.

Técnicamente, el ACP busca la proyección según la cual los datos queden mejor representados en términos de mínimos cuadrados. Esta convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables sin correlación lineal llamadas componentes principales.

El ACP se emplea sobre todo en análisis exploratorio de datos y para construir modelos predictivos. El ACP comporta el cálculo de la descomposición en autovalores de la matriz de covarianza, normalmente tras centrar los datos en la media de cada atributo.

Son 2 los principales objetivos de la reducción de dimensiones tambien llamado analisis de componentes principales: 

	1. Encontrar un nuevo subconjunto de variables que no estén correlacionadas y que expliquen el maximo de varianza de los datos posible.
		* normmalmente muchas de las variables no son independientes  (i.e. height vs weight)
		* es un problema estadístico que resuelve el **PCA**.
	2. Encontrar el menor rango de una matriz ( = la mejor matriz que con menor numero de variables) que explique los datos.. explique la varianza de los datos. 
		* es un problema de compresión de los datos --> **Singular Value Decomposition (SVD)**

* ***Ejemplo***
Vamos a crear artificialmente unos datos que tienen un patrón. Por ejemplo tiramos una moneda y si sale cara remplazamos lo datos con ceros y treses [0, 0, 0, 0, 0, 3, 3, 3, 3, 3].

```{r fig.height = 3, fig.width = 7, fig.align='center'}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip){
    data[i,] <- data[i,] + rep(c(0,3),each=5)
  }
}
# hierarchical clustering
hh <- hclust(dist(data))

plot(hh)
heatmap(as.matrix(data))

#???ordenamos las filas por orden ascendente:
dataOrdered <- data[hh$order,]
heatmap(dataOrdered)
# create 1 x 3 panel plot
par(mfrow=c(1,3))
# heat map (sorted)
image(t(dataOrdered)[,nrow(dataOrdered):1])
# row means (40 rows)
plot(rowMeans(dataOrdered),40:1,xlab="Row Mean",ylab="Row",pch=19)
# column means (10 columns)
plot(colMeans(dataOrdered),xlab="Column",ylab="Column Mean",pch=19)
```

# PRINCIPIOS MATEMÁTICOS

## SVD
Descomposición en valores singulares
O como se dice en inglés Singular Value Decomposition (SVD) de una matriz es una factorización de la misma con muchas aplicaciones en estadística y otras disciplinas.

Sea $X$ una matriz que contiene en cada columna una variables y en cada fila una observación. LA descomposición de la matiz consisten en dividir esta matriz en 3 matrices separadas de la frma siguiente $$X = UDV^T$$.Donde:

	- $U$ = left singular vector, orthogonal matrix (columns independent of each other)
	- $D$ = singular values, diagonal matrix
	- $V$ = right singular vector, orthogonal matrix (columns independent of each other)
	- ***Note**: orthogonal implies that a matrix is always invertible [$A^{-1} = A^T$] and that the product of the matrix and its transpose equals the identity matrix [$AA^T = I$] *
		+ when a orthogonal matrices, $A$, is multiplied by another matrix, $B$, it is effectively a linear transformation in that the length and angles of $B$ are preserved
	- ***Note**: diagonal implies that any value outside of the main diagonal ($\searrow$) = 0 *
		+ example $$A = \begin{bmatrix}
       1 & 0 & 0 \\
       0 & 2 & 0 \\
       0 & 0 & 3 \end{bmatrix}$$
* ***Note**: scale of data matters for SVD/PCA (scaling the data may help), patterns detected maybe mixed together, and computation is intensive for these operations *

## Principal Components Analysis (PCA)
* first scale the variables and run SVD on normalized matrix
	* **scaling** = subtract each column by its mean and divide by its standard deviation
* **principal components** = the right singular values or the $V$ matrix

### SVD y PCA Ejemplos
* **$U$ y $V$ son matrices**
	- `s <- svd(data)` = realiza un SVD en los datos ($n \times m$ matrix) y los separa en $u$, $v$, y $d$ matrices.
		+ `s$u` = $n \times m$ matrix $\rightarrow$ variación horizontal
		+ `s$d` = $1 \times m$ vector $\rightarrow$ vector de los valores singulares/diagonal.
			* `diag(s$d)` = $m \times m$ matriz diagonal 
		+ `s$v` = $m \times m$ matrix $\rightarrow$ variación vertical
		+ `s$u %*% diag(s$d) %*% t(s$v)` = returns the original data $\rightarrow$ $X = UDV^T$
	- `scale(data)` = escala los datos originales sustrayendo la media por columna, y dividiendo por la desviación estandar por columna.

El resultado es similar a las graficas que hemosvisto antes:

```{r fig.height=3,fig.width=7, fig.align = 'center'}
# running svd
svd1 <- svd(scale(dataOrdered))
# create 1 by 3 panel plot
par(mfrow=c(1,3), mar=c(2,4,0.1,0.1))
# data heatmap (sorted)
image(t(dataOrdered)[,nrow(dataOrdered):1])
# U Matrix - first column
plot(svd1$u[,1],40:1,xlab="Row",ylab="First left singular vector",pch=19)
# V vector - first column
plot(svd1$v[,1],xlab="Column",ylab="First right singular vector",pch=19)
```

Para explicar la variación de la varianza de una descomposición SDV tenemos como resultado el vector $D$. este vercor nos da la explicación en % de la varianza de los datos en cada variable o columna. Lo que es equivalente a normalizar los valores singulares y convertirlos en procentaje.

	* $d$ matrix (`s$d` vector) captures the singular values, or ***variation in data that is explained by that particular component*** (variable/column/dimension)


en el ejemplo anterior el primer valor explica el 40% de la varización de los datos.

```{r fig.height=3,fig.width=5, fig.align = 'center'}
# create 1 x 2 panel plot
par(mfrow=c(1,2))
# plot singular values
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
# plot proportion of variance explained
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Prop. of variance explained",pch=19)
```

El analisis de SVD y el PCA son básicamente la misma cosa, y lo demuestra con la siguente representación de los resultados:

* **Relationship to PCA**
	* `p <- prcomp(data, scale = TRUE)` = performs PCA on data specified
		- `scale = TRUE` = scales the data before performing PCA
		- returns `prcomp` object
		- `summary(p)` = prints out the principal component's standard deviation, proportion of variance, and cumulative proportion
	* PCA's rotation vectors are equivalent to their counterparts in the V matrix from the SVD

```{r fig.height=4,fig.width=4, fig.align='center'}
# SVD
svd1 <- svd(scale(dataOrdered))
# PCA
pca1 <- prcomp(dataOrdered,scale=TRUE)
# Plot the rotation from PCA (Principal Components) vs v vector from SVD
plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab="Principal Component 1",
	ylab="Right Singular Vector 1")
abline(c(0,1))
# summarize PCA
summary(pca1)
```
## relaciones complejas en los datos
para buscar patrones complejos  y detectarlos es importante seguir una pautas antes de la comparación de SVD o PVA. 
Lo primero es escalar los datos de forma que la varianza de cada col sea similar, es decir lo mejor sería normalizar los valores para que la comparación fuera coherente.

* **More Complex Patterns**
	* SVD can be used to ***detect unknown patterns*** within the data (we rarely know the true distribution/pattern about the population we're analyzing)
	* however, it may be hard to pinpoint exact patterns as the principal components may confound each other
		- in the example below, you can see that the two principal components that capture the most variation have both horizontal shifts and alternating patterns captured in them


```{r ,fig.height=6,fig.width=7, fig.align='center'}
set.seed(678910)
# setting pattern
data <- matrix(rnorm(400), nrow = 40)
for(i in 1:40){
  # flip a coin
  coinFlip1 <- rbinom(1,size=1,prob=0.5)
  coinFlip2 <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip1){
    data[i,] <- data[i,] + rep(c(0,5),each=5)
  }
  if(coinFlip2){
    data[i,] <- data[i,] + rep(c(0,5),5)
  }
}
hh <- hclust(dist(data)); dataOrdered <- data[hh$order,]

# perform SVD
svd2 <- svd(scale(dataOrdered))
par(mfrow=c(2,3))
image(t(dataOrdered)[,nrow(dataOrdered):1])
plot(rep(c(0,1),each=5),pch=19,xlab="Column", main="True Pattern 1")
plot(rep(c(0,1),5),pch=19,xlab="Column",main="True Pattern 2")
image(t(dataOrdered)[,nrow(dataOrdered):1])
plot(svd2$v[,1],pch=19,xlab="Column",ylab="First right singular vector",
	main="Detected Pattern 1")
plot(svd2$v[,2],pch=19,xlab="Column",ylab="Second right singular vector",
	main="Detected Pattern 2")
```

### Ejemplo
Creamos una matriz de solo ceros y unos.
Hacemos el analisis SVD y vemos que toda la variazión es explicada por el primer componente principal como es lógico.
```{r}
constantMatrix<- matrix(rep(0,400), nrow = 40) #(dim 40x10)
#creo una matriz que repite 5 ceros y 5 unos por fila 
for(i in 1:dim(constantMatrix)[1]) {constantMatrix[i,]<-rep(c(0,1), each=5) }

#realizo el analisis SVD
svd1<-svd(constantMatrix)
par(mfrow=c(1,3))

image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d, xlab="col", ylab="singula valor",pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="col", ylab="Prob. explicacion de varianza",pch=19)

```


## NA. Como tratar las ausencias de datos

Antes de proceder con un estudio de componentes principales SVD tenemos que limpiar la matriz para eliminar los valores NA ausentes, ya que no se puede realizar el analisis con ausencia de datos en la matriz origen.

Una solución es usar el paquete  `impute` de  [Bioconductor](http://bioconductor.org). Esto nos permite aproximar o asignar a los valores ausentes una media de los cercanos (como un krigging).

La función `impute.knn` toma la observación sin dato y le asigna el valor  teniendo en cuenta las `k` observaciones más cercanas a esa fila ( el valor por defecto es `k=10`).

```{r fig.height=3,fig.width=5, fig.align='center', eval=FALSE}
#install.packages("impute")
library(impute)  ## Available from http://bioconductor.org
data2 <- matrix(rnorm(400), nrow = 40)
# set random samples = NA
data2[sample(1:100,size=40,replace=FALSE)] <- NA
data2 <- impute.knn(data2)$data
svd1 <- svd(scale(dataOrdered)); svd2 <- svd(scale(data2))
par(mfrow=c(1,2))
plot(svd1$v[,1],pch=19, main="Original")
plot(svd2$v[,1],pch=19, main="Imputed")
```

## Ejemplo de SVD con imagen
en este ejemplo vremos como funciona la reduccion de dimensiones SVD para -por ejemplo -comprimir una imagen real:

* if we look at the variance explained plot below, ***most of the variation*** is explained by the ***first few principal components***

```{r fig.height=3,fig.width=4, fig.align='center', eval=FALSE}
# load faceData
load("figures/face.rda")
# perform SVD
svd3 <- svd(scale(faceData))
plot(svd3$d^2/sum(svd3$d^2),pch=19,xlab="Singular vector",ylab="Variance explained")
```

* approximations can thus be created by taking the first few components and using matrix multiplication with the corresponding $U$, $V$, and $D$ components

```{r fig.height=2.5,fig.width=7, fig.align='center' , eval=FALSE}
approx1 <- svd3$u[,1] %*% t(svd3$v[,1]) * svd3$d[1]
approx5 <- svd3$u[,1:5] %*% diag(svd3$d[1:5])%*% t(svd3$v[,1:5])
approx10 <- svd3$u[,1:10] %*% diag(svd3$d[1:10])%*% t(svd3$v[,1:10])
# create 1 x 4 panel plot
par(mfrow=c(1,4))
# plot original facedata
image(t(approx1)[,nrow(approx1):1], main = "1 Component")
image(t(approx5)[,nrow(approx5):1], main = "5 Component")
image(t(approx10)[,nrow(approx10):1], main = "10 Component")
image(t(faceData)[,nrow(faceData):1], main = "Original")
```


