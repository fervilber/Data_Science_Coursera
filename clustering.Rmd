---
title: "Clustering"
author: "Fernando Villalba"
date: "13 de febrero de 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Agrupamiento de datos

Un algoritmo de agrupamiento (en inglés, **clustering**) es un procedimiento de agrupación de una serie de vectores de acuerdo con un criterio. Esos criterios son por lo general distancia o similitud. La cercanía se define en términos de una determinada función de distancia, como la euclídea, aunque existen otras más robustas o que permiten extenderla a variables discretas. La medida más utilizada para medir la similitud entre los casos es la matriz de correlación entre los n x n casos. Sin embargo, también existen muchos algoritmos que se basan en la máximización de una propiedad estadística llamada verosimilitud.

Generalmente, los vectores de un mismo grupo (o clústers) comparten propiedades comunes.El conocimiento de los grupos puede permitir una descripción sintética de un conjunto de datos multidimensional complejo. De ahí su uso en minería de datos. Esta descripción sintética se consigue sustituyendo la descripción de todos los elementos de un grupo por la de un representante característico del mismo.



# Agrupamiento jerárquico (Hierarchical Clustering)

En minería de datos, el agrupamiento jerárquico (**Hierarchical Clustering**) es un método de análisis que busca construir una **jerarquía** de grupos de forma automatizada a partir de lo juntos que estén los datos según funciones de distancia.

Es util para ver datos de alta dimensionalidad, consiste en agrupar los datos que están juntos en grupos.

* **agglomerative approach** (most common) â€” bottom up
	1. start with data
	2. find closest pairs, put them together (create "super point" and remove original data)
	3. find the next closest
	4. repeat = yields a tree showing order of merging (dendrogram)
	* requires
		* ***merging approach***: how to merge two points
		* ***distance metric***: calculating distance between two points
		* **continuous** - *Euclidean distance* $\rightarrow$ $\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + \dots + (Z_1 -Z_2)^2 }$
		* **continuous** - *correlation similarity* $\rightarrow$ how correlated two data points are
		* **binary** - *Manhattan distance* ("city block distance") $\rightarrow$ $|A_1 - A_2| + |B_1 - B_2| + \dots + |Z_1 -Z_2|$

```{r fig.height = 3, fig.width = 6, fig.align='center', echo=FALSE}
install.packages("png")
library(png)
library(grid)
setwd("C:/R/proyectos/Data_Science_Coursera/04_EXPLORAR_Y_ANALIZAR_DATOS/")
grid.raster(readPNG("figures/25.png"))
```

## Procedimiento para construir un Hierarchical Clusters (funcion `hclust`)
El proceso es simple:

1. calculamos la distancia entre cada par de puntos para ver  cual está más cerca
	* `dist(data.frame(x=x, y=y)` = returns pair wise distances for all of the (x,y) coordinates
	* ***Note**: `dist()` function uses Euclidean distance by default *
2. Agrupamos los dos puntos más cercanos en base a la tabla de dsitancia anterior y los sustituimos por un único punto agrupado.
3. Volvemos al paso anterior buscando los siguientes 2 puntos más cercanos y los juntamos .. y así hasta juntar todos los puntos en uno
4. El orden del agrupamiento se muestra en un dendrograma

Un **dendrograma** es un tipo de representación gráfica o diagrama de datos en forma de árbol que organiza los datos en subcategorías que se van dividiendo en otros hasta llegar al nivel de detalle deseado.

Observando las sucesivas subdivisiones podemos hacernos una idea sobre los criterios de agrupación de los mismos, la distancia entre los datos según las relaciones establecidas, etc.
Hay que tener presenta varias cosas, primero que las agrupaciones son gráficos inestables en el sentido que pequeños cambios en los puntos pueden producir grandes cambios en la froma de agruparse. 

También que el uso de diferentes métricas (funciones de distancia) conlleva diferentes resultados.
Por otro lado las salidas de la funcion `hclust` son deterministas, en el sentido que los mismos argumentos de entrada, producen siempre la misma salida o resultado.
Determinar cuantos cluster o agrupamientos hay no siempre es sencillo, pero son graficos apropiados para un analisis exploatorio de datos, que pueden permitirnos comprobar de un vistazo si existe o no patrones de comportamiento en los datos.

### `hclust` Función 

La funcion `hclust` puede utilizar dos argumento para `method = "complete"` o `"method = average"`. Podemos traducirlos como vinculación completa o promedio.

* ***average linkage*** = toma la distancia media ... centro de masas de los puntos del cluster
* ***complete linkage*** = toma la distancia más alejada entre los puntos posible
* ***Note**: cada aproximación prodice resultados diferentes*


* `hh <- hclust(dist(dataFrame))` función = produce un **objeto** agrupamiento jerarquico de los datos basado en pares de distancias entre los datos de un data frame con valores x e y.
	* `dist()` = por defecto es distancia Euclidea, calcula la distancia o similitud entre cada 2 observaciones de los datos; cuando se aplica sobre un data frame, la funcion aplica esta formula a cada par de datos: $\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + ... + (Z_1 -Z_2)^2 }$ .Construye como resultado una matriz de distancias entre puntos (por fila- observaciones).
		* El orden del agrupamiento se determina por estas distancias
	* `plot(hh)` = pinbta el dendrograma
	* Ordena automáticamente las columnas y filas
	* `names(hh)` = devuelve todos los nombres del objeto `hclust`
		* `hh$order` = devuelve el orden de las filas/clusters del dendrograma
		* `hh$dist.method` = devuelve el metodo usado para el calculo de la distancia o similitud entre puntos.
* ***Note**: El **dendrograma no dice cuantos cluster hay** , para saber esto hay que hacer un corte en un nivel del mismo.

### `hclust` Ejemplos 
El proceso es: 

1. calcular la distancia.
2. calcular el dendrograma.

```{r fig.height = 3, fig.width = 4, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
#pintamos los puntos
plot(x,y,col="blue",pch=19)
text(x = x, y = y, labels = rownames(dataFrame), pos=4, col="red")

#calculamos la distancia
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
#hClustering <- hclust(distxy, method = "complete")
#hClustering <- hclust(distxy, method = "average")
plot(hClustering)
```


### `myplcclust` Función

La función `myplcclust`, cuyo código se muestra a continuación, es una variante de `hclust`, en la que se pinta y etiqueta cada cluster con un número y un color. 
Hay que saber como argumento el número de clustres inicial, pr lo que debemos antes ejecutar `hclust`

```{r}
myplclust <- function(hclust, lab = hclust$labels,
	lab.col = rep(1, length(hclust$labels)), hang = 0.1, ...) {
	## modifiction of plclust for plotting hclust objects *in colour*! Copyright
	## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector
	## of labels of the leaves of the tree lab.col: colour for the labels;
	## NA=default device foreground colour hang: as in hclust & plclust Side
	## effect: A display of hierarchical cluster with coloured leaf labels.
	y <- rep(hclust$height, 2)
	x <- as.numeric(hclust$merge)
	y <- y[which(x < 0)]
	x <- x[which(x < 0)]
	x <- abs(x)
	y <- y[order(x)]
	x <- x[order(x)]
	plot(hclust, labels = FALSE, hang = hang, ...)
	text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
 col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)
}
```

Veamos un ejemplo:

```{r fig.height = 3, fig.width = 4, fig.align='center'}
# example
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

### `heatmap` Función
Otra funcion interesante para representar el agrupamiento es `heatmap(data.matrix)`, que es una función similar a `image(t(x))`. Es una buena forma de representar gráficos multidimensionales, de muchas dimensiones que toman de base una matriz multidimensional.
El argumento debe ser una matriz, por lo que es util `as.matrix(data.frame)` para convertir en caso necesario.

Lo que hace es realizar el agrupamiento jerarquico no solo por observaciones o filas como `hclust`, sino tambien en las columnas o variables, por lo que es como tomar las variables como subconjuntos de observaciones y vemos en una simple imagen qué variables son más cercanas o están más relacionadas que otras.
Esto tiene sentido con matrices de muchas variables, no para una de solo dos x,y, pues no dice la relacion entre columnas.

EL mapa final se muestra en dos colores, el amarillo indican relación alta y el rojo relación a la baja o contraria

```{r fig.height = 3, fig.width = 4, fig.align='center'}
set.seed(12345)
data <- matrix(rnorm(400), nrow = 40)
# data tiene 40 filas y 10 col o variables
heatmap(data)

```

```{r}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
y1<- y*2
y2<-y1+.5
dataFrame <- data.frame(x,y,y1,y2)
heatmap(as.matrix(dataFrame))
```

### `image` Función
La función `image()` produce una salida identica a `heatmap()`,pero sin los dendrogramas.
* `image(x, y, t(dataMatrix)[, nrow(dataMatrix):1])` 
	* `t(dataMatrix)[, nrow(dataMatrix)]`
		* `t(dataMatrix)` = transpose of dataMatrix, this is such that the plot will be displayed in the same fashion as the matrix (rows as values on the y axis and columns as values on the x axis)
			* ***example*** 40 x 10 matrix will have graph the 10 columns as x values and 40 rows as y values
		* `[, nrow(dataMatrix)]` = subsets the data frame in reverse column order; when combined with the `t()` function, it reorders the rows of data from 40 to 1, such that the data from the matrix is displayed in order from top to bottom
			* ***Note**: without this statement the rows will be displayed in order from bottom to top, as that is in line with the positive y axis *
	* `x`, `y` = used to specify the values displayed on the x and y axis
		* ***Note**: must be in increasing order *
* ***example***

```{r fig.height = 4, fig.width = 3, fig.align='center'}
set.seed(12345)
data <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data)[, nrow(data):1])
```

$\pagebreak$

## K-means Clustering

Otra técnica de agrupamiento, muy antigua, es el llamado k-mean clustering, que en el fondo es similar al agrupamiento jerarquico anterior, que proporciona una manera bastante eficiente de resumir datos multidimensionales y ver si existen patrones entre ellos, si hay similitud en las observaciones etc.

K-means es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Es un método utilizado en minería de datos.

Lo principal del agrupamiento es la definición de qué significa cercanía, qué es lo que está cerca y qué lejos.
La forma de calcular esta distancia es la clave. Como sabemos la manera más intuitiva es definir la distancia física entre dos puntos, pero podemos considerar tambien la cercanía o no según la correlación de dos variables..otra opción es considerar la distancia Manhattan, para reticulas..

El algoritmo K-mean parte del conocimiento del número de grupos inicial que hay, y de unas coordenadas inciales para cada centroide de cada grupo.

Iterativamente va a asignar los puntos a un grupo por cercanía al centroide, y recalcula el centroide con los puntos de agrupados...y así hasta que localiza el centro de cada agrupación.  

Este algoritmo **no es deterministico**, pues parte de una agrupacion incial estimada y puede ofrecer diferentes resultados para los mismos parámetros iniciales.


###  Funcion (`kmeans`)
* **Proceso de cálculo del algoritmo K-Means**
	1. Establece el numero inicial de grupos (o clusters)
	2. Busca el centroide de cada uno
	3. asigna los puntos al centroide más cercano.
	4. recalcula el centroide
	5. repite = vuelve alpaso 2.. y así
	* **requiere**
		* definir la distancia
		* numero inicial de clusters
		* sugerir la posición inicial de cada centroide para cada agrupación

* ***Ejemplo***

```{r fig.height = 2, fig.width = 3, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
# specifies initial number of clusters to be 3
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj) # nos da los argumentos posibles de kmeans
# returns cluster assignments
kmeansObj$cluster #calcula los clusters
#para pintar los resultados
par(mar=rep(0.2,4))
#pintamos los puntos primero
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
#despues pintamos los centroides de los clusters
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```

**Ejemplo 2**
```{r fig.height = 2, fig.width = 3, fig.align='center'}
set.seed(1234)
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <- data.frame(x=x,y=y)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2<-kmeans(dataMatrix, centers = 3)

par(mfrow=c(1,2), mar=c(2,4,0.1,0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1], yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)], yaxt="n")


dataFrame <- data.frame(x=x,y=y)
# specifies initial number of clusters to be 3
kmeansObj <- kmeans(dataFrame,centers=3)
```


$\pagebreak$

# Análisis de componentes principales

En estadística, el análisis de componentes principales (en español ACP, en inglés, PCA) es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos.

Técnicamente, el ACP busca la proyección según la cual los datos queden mejor representados en términos de mínimos cuadrados. Esta convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables sin correlación lineal llamadas componentes principales.

El ACP se emplea sobre todo en análisis exploratorio de datos y para construir modelos predictivos. El ACP comporta el cálculo de la descomposición en autovalores de la matriz de covarianza, normalmente tras centrar los datos en la media de cada atributo.

Son 2 los principales objetivos de la reducción de dimensiones tambien llamado analisis de componentes principales: 

	1. Encontrar un nuevo subconjunto de variables que no estén correlacionadas y que expliquen el maximo de varianza de los datos posible.
		* normmalmente muchas de las variables no son independientes  (i.e. height vs weight)
		* es un problema estadístico que resuelve el PCA.
	2. Encontrar el menor rango de una matriz ( = la mejor matriz que con menor numero de variables) que explique los datos.. explique la varianza de los datos. 
		* es un problema de compresión de los datos -->Singular Value Decomposition (SVD)

* ***Ejemplo***
Vamos a crear artificialmente unos datos que tienen un patrón. Por ejemplo tiramos una moneda y si sale cara remplazamos lo datos con ceros y treses [0, 0, 0, 0, 0, 3, 3, 3, 3, 3].

```{r fig.height = 3, fig.width = 7, fig.align='center'}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip){
    data[i,] <- data[i,] + rep(c(0,3),each=5)
  }
}
# hierarchical clustering
hh <- hclust(dist(data))

plot(hh)
heatmap(as.matrix(data))

#???ordenamos las filas por orden ascendente:
dataOrdered <- data[hh$order,]
heatmap(dataOrdered)
# create 1 x 3 panel plot
par(mfrow=c(1,3))
# heat map (sorted)
image(t(dataOrdered)[,nrow(dataOrdered):1])
# row means (40 rows)
plot(rowMeans(dataOrdered),40:1,xlab="Row Mean",ylab="Row",pch=19)
# column means (10 columns)
plot(colMeans(dataOrdered),xlab="Column",ylab="Column Mean",pch=19)
```

## Descomposición en valores singulares
O como se dice en inglés Singular Value Decomposition (SVD) de una matriz es una factorización de la misma con muchas aplicaciones en estadística y otras disciplinas.

Sea $X$ una matriz que contiene en cada columna una variables y en cada fila una observación. LA descomposición de la matiz consisten en dividir esta matriz en 3 matrices separadas de la frma siguiente $$X = UDV^T$$.Donde:

	- $U$ = left singular vector, orthogonal matrix (columns independent of each other)
	- $D$ = singular values, diagonal matrix
	- $V$ = right singular vector, orthogonal matrix (columns independent of each other)
	- ***Note**: orthogonal implies that a matrix is always invertible [$A^{-1} = A^T$] and that the product of the matrix and its transpose equals the identity matrix [$AA^T = I$] *
		+ when a orthogonal matrices, $A$, is multiplied by another matrix, $B$, it is effectively a linear transformation in that the length and angles of $B$ are preserved
	- ***Note**: diagonal implies that any value outside of the main diagonal ($\searrow$) = 0 *
		+ example $$A = \begin{bmatrix}
       1 & 0 & 0 \\
       0 & 2 & 0 \\
       0 & 0 & 3 \end{bmatrix}$$
* ***Note**: scale of data matters for SVD/PCA (scaling the data may help), patterns detected maybe mixed together, and computation is intensive for these operations *

### Principal Components Analysis (PCA)
* first scale the variables and run SVD on normalized matrix
	* **scaling** = subtract each column by its mean and divide by its standard deviation
* **principal components** = the right singular values or the $V$ matrix

### SVD and PCA Example
* **$U$ and $V$ Matrices**
	- `s <- svd(data)` = performs SVD on data ($n \times m$ matrix) and splits it into $u$, $v$, and $d$ matrices
		+ `s$u` = $n \times m$ matrix $\rightarrow$ horizontal variation
		+ `s$d` = $1 \times m$ vector $\rightarrow$ vector of the singular/diagonal values
			* `diag(s$d)` = $m \times m$ diagonal matrix
		+ `s$v` = $m \times m$ matrix $\rightarrow$ vertical variation
		+ `s$u %*% diag(s$d) %*% t(s$v)` = returns the original data $\rightarrow$ $X = UDV^T$
	- `scale(data)` = scales the original data by subtracting each data point by its column mean and dividing by its column standard deviation

```{r fig.height=3,fig.width=7, fig.align = 'center'}
# running svd
svd1 <- svd(scale(dataOrdered))
# create 1 by 3 panel plot
par(mfrow=c(1,3))
# data heatmap (sorted)
image(t(dataOrdered)[,nrow(dataOrdered):1])
# U Matrix - first column
plot(svd1$u[,1],40:1,,xlab="Row",ylab="First left singular vector",pch=19)
# V vector - first column
plot(svd1$v[,1],xlab="Column",ylab="First right singular vector",pch=19)
```

* **$D$ Matrix and Variance Explained**
	* $d$ matrix (`s$d` vector) captures the singular values, or ***variation in data that is explained by that particular component*** (variable/column/dimension)
	* **proportion of variance Explained** = converting the singular values to variance (square the values) and divide by the total variance (sum of the squared singular values)
		- effectively the same pattern as the singular values, just converted to percentage
		- in this case, the first component/dimension, which captures the shift in means (see previous plot) of SVD captures about 40% of the variation

```{r fig.height=3,fig.width=5, fig.align = 'center'}
# create 1 x 2 panel plot
par(mfrow=c(1,2))
# plot singular values
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
# plot proportion of variance explained
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Prop. of variance explained",pch=19)
```

* **Relationship to PCA**
	* `p <- prcomp(data, scale = TRUE)` = performs PCA on data specified
		- `scale = TRUE` = scales the data before performing PCA
		- returns `prcomp` object
		- `summary(p)` = prints out the principal component's standard deviation, proportion of variance, and cumulative proportion
	* PCA's rotation vectors are equivalent to their counterparts in the V matrix from the SVD

```{r fig.height=4,fig.width=4, fig.align='center'}
# SVD
svd1 <- svd(scale(dataOrdered))
# PCA
pca1 <- prcomp(dataOrdered,scale=TRUE)
# Plot the rotation from PCA (Principal Components) vs v vector from SVD
plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab="Principal Component 1",
	ylab="Right Singular Vector 1")
abline(c(0,1))
# summarize PCA
summary(pca1)
```

* **More Complex Patterns**
	* SVD can be used to ***detect unknown patterns*** within the data (we rarely know the true distribution/pattern about the population we're analyzing)
	* however, it may be hard to pinpoint exact patterns as the principal components may confound each other
		- in the example below, you can see that the two principal components that capture the most variation have both horizontal shifts and alternating patterns captured in them


```{r ,fig.height=6,fig.width=7, fig.align='center'}
set.seed(678910)
# setting pattern
data <- matrix(rnorm(400), nrow = 40)
for(i in 1:40){
  # flip a coin
  coinFlip1 <- rbinom(1,size=1,prob=0.5)
  coinFlip2 <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip1){
    data[i,] <- data[i,] + rep(c(0,5),each=5)
  }
  if(coinFlip2){
    data[i,] <- data[i,] + rep(c(0,5),5)
  }
}
hh <- hclust(dist(data)); dataOrdered <- data[hh$order,]

# perform SVD
svd2 <- svd(scale(dataOrdered))
par(mfrow=c(2,3))
image(t(dataOrdered)[,nrow(dataOrdered):1])
plot(rep(c(0,1),each=5),pch=19,xlab="Column", main="True Pattern 1")
plot(rep(c(0,1),5),pch=19,xlab="Column",main="True Pattern 2")
image(t(dataOrdered)[,nrow(dataOrdered):1])
plot(svd2$v[,1],pch=19,xlab="Column",ylab="First right singular vector",
	main="Detected Pattern 1")
plot(svd2$v[,2],pch=19,xlab="Column",ylab="Second right singular vector",
	main="Detected Pattern 2")
```

* **Missing Data**
	* SVD cannot be performed on dataset with `NA` values
	* `impute` package from [Bioconductor](http://bioconductor.org) can help approximate missing values from surrounding values
		- `impute.knn` function takes the missing row and imputes the data using the `k` nearest neighbors to that row
			+ `k=10` = default value (take the nearest 10 rows)

```{r fig.height=3,fig.width=5, fig.align='center'}
library(impute)  ## Available from http://bioconductor.org
data2 <- dataOrdered
# set random samples = NA
data2[sample(1:100,size=40,replace=FALSE)] <- NA
data2 <- impute.knn(data2)$data
svd1 <- svd(scale(dataOrdered)); svd2 <- svd(scale(data2))
par(mfrow=c(1,2))
plot(svd1$v[,1],pch=19, main="Original")
plot(svd2$v[,1],pch=19, main="Imputed")
```

### Create Approximations/Data Compression
* SVD can be used to create lower rank representation, or compressed representation of data
* if we look at the variance explained plot below, ***most of the variation*** is explained by the ***first few principal components***

```{r fig.height=3,fig.width=4, fig.align='center'}
# load faceData
load("figures/face.rda")
# perform SVD
svd3 <- svd(scale(faceData))
plot(svd3$d^2/sum(svd3$d^2),pch=19,xlab="Singular vector",ylab="Variance explained")
```

* approximations can thus be created by taking the first few components and using matrix multiplication with the corresponding $U$, $V$, and $D$ components

```{r fig.height=2.5,fig.width=7, fig.align='center'}
approx1 <- svd3$u[,1] %*% t(svd3$v[,1]) * svd3$d[1]
approx5 <- svd3$u[,1:5] %*% diag(svd3$d[1:5])%*% t(svd3$v[,1:5])
approx10 <- svd3$u[,1:10] %*% diag(svd3$d[1:10])%*% t(svd3$v[,1:10])
# create 1 x 4 panel plot
par(mfrow=c(1,4))
# plot original facedata
image(t(approx1)[,nrow(approx1):1], main = "1 Component")
image(t(approx5)[,nrow(approx5):1], main = "5 Component")
image(t(approx10)[,nrow(approx10):1], main = "10 Component")
image(t(faceData)[,nrow(faceData):1], main = "Original")
```


$\pagebreak$

## Color Packages in R Plots
* proper use of color can help convey the message by improving clarity/contrast of data presented
* default color schemes for most plots in R are fairly terrible, so some external packages are helpful

### `grDevices` Package
* `colors()` function = lists names of colors available in any plotting function
* **`colorRamp` function**
	- takes any set of colors and return a function that takes values between 0 and 1, indicating the extremes of the color palette (e.g. see the `gray` function)
	- `pal <- colorRamp(c("red", "blue"))` = defines a `colorRamp` function
	- `pal(0)` returns a 1 x 3 matrix containing values for RED, GREEN, and BLUE values that range from 0 to 255
	- `pal(seq(0, 1, len = 10))` returns a 10 x 3 matrix of 10 colors that range from RED to BLUE (two ends of spectrum defined in the object)
	- ***example***

```{r}
# define colorRamp function
pal <- colorRamp(c("red", "blue"))
# create a color
pal(0.67)
```

* **`colorRampPalette` function**
	- takes any set of colors and return a function that takes integer arguments and returns a vector of colors interpolating the palette (like `heat.colors` or `topo.colors`)
	- `pal <- colorRampPalette(c("red", "yellow"))` defines a `colorRampPalette` function
	- `pal(10)` returns 10 interpolated colors in hexadecimal format that range between the defined ends of spectrum
	- ***example***

```{r}
# define colorRampPalette function
pal <- colorRampPalette(c("red", "yellow"))
# create 10 colors
pal(10)
```

* **`rgb` function**
	- `red`, `green`, and `blue` arguments = values between 0 and 1
	- `alpha = 0.5` = transparency control, values between 0 and 1
	- returns hexadecimal string for color that can be used in `plot`/`image` commands
	- `colorspace` package `cna` be used for different control over colors
	- ***example***

```{r fig.height = 4, fig.width = 6, fig.align='center'}
x <- rnorm(200); y <- rnorm(200)
par(mfrow=c(1,2))
# normal scatter plot
plot(x, y, pch = 19, main = "Default")
# using transparency shows data much better
plot(x, y, col = rgb(0, 0, 0, 0.2), main = "With Transparency")
```

### `RColorBrewer` Package
* can be found on CRAN that has predefined color palettes
	* `library(RColorBrewer)`
* **types of palettes**
	- *Sequential* = numerical/continuous data that is ordered from low to high
	- *Diverging* = data that deviate from a value, increasing in two directions (i.e. standard deviations from the mean)
	- *Qualitative* = categorical data/factor variables
* palette information from the `RColorBrewer` package can be used by `colorRamp` and `colorRampPalette` functions
* **available colors palettes**

```{r fig.height = 6, fig.width = 8, fig.align='center', echo=FALSE}
grid.raster(readPNG("figures/color.png"))
```

* **`brewer.pal(n, "BuGn")` function**
	- `n` = number of colors to generated
	- `"BuGn"` = name of palette
		+ `?brewer.pal` list all available palettes to use
	- returns list of `n` hexadecimal colors
* ***example***

```{r fig.height = 3, fig.width = 7, fig.align='center'}
library(RColorBrewer)
# generate 3 colors using brewer.pal function
cols <- brewer.pal(3, "BuGn")
pal <- colorRampPalette(cols)
par(mfrow=c(1,3))
# heat.colors/default
image(volcano, main = "Heat.colors/Default")
# topographical colors
image(volcano, col = topo.colors(20), main = "Topographical Colors")
# RColorBrewer colors
image(volcano, col = pal(20), main = "RColorBrewer Colors")
```

* **`smoothScatter` function**
	- used to plot large quantities of data points
	- creates 2D histogram of points and plots the histogram
	- default color scheme = "Blues" palette from `RColorBrewer` package
	- ***example***

```{r fig.height = 4, fig.width = 4, fig.align='center'}
x <- rnorm(10000); y <- rnorm(10000)
smoothScatter(x, y)
```


$\pagebreak$

## Case Study: Human Activity Tracking with Smart Phones

**Loading Training Set of Samsung S2 Data from [UCI Repository](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)**


```{r}
# load data frame provided
load("samsungData.rda")
# table of 6 types of activities
table(samsungData$activity)
```

**Plotting Average Acceleration for First Subject**

```{r fig.height=4,fig.width=7, fig.align='center'}
# set up 1 x 2 panel plot
par(mfrow=c(1, 2), mar = c(5, 4, 1, 1))
# converts activity to a factor variable
samsungData <- transform(samsungData, activity = factor(activity))
# find only the subject 1 data
sub1 <- subset(samsungData, subject == 1)
# plot mean body acceleration in X direction
plot(sub1[, 1], col = sub1$activity, ylab = names(sub1)[1],
	main = "Mean Body Acceleration for X")
# plot mean body acceleration in Y direction
plot(sub1[, 2], col = sub1$activity, ylab = names(sub1)[2],
	main = "Mean Body Acceleration for Y")
# add legend
legend("bottomright",legend=unique(sub1$activity),col=unique(sub1$activity), pch = 1)
```


**Clustering Based on Only Average Acceleration**

```{r fig.height=5,fig.width=7, fig.align='center'}
# load myplclust function
source("myplclust.R")
# calculate distance matrix
distanceMatrix <- dist(sub1[,1:3])
# form hclust object
hclustering <- hclust(distanceMatrix)
# run myplclust on data
myplclust(hclustering, lab.col = unclass(sub1$activity))
```


**Plotting Max Acceleration for the First Subject**

```{r fig.height=4,fig.width=7, fig.align='center'}
# create 1 x 2 panel
par(mfrow=c(1,2))
# plot max accelecrations in x and y direction
plot(sub1[,10],pch=19,col=sub1$activity,ylab=names(sub1)[10],
	main = "Max Body Acceleration for X")
plot(sub1[,11],pch=19,col = sub1$activity,ylab=names(sub1)[11],
	main = "Max Body Acceleration for Y")
```


**Clustering Based on Maximum Acceleration**

```{r fig.height=5,fig.width=7, fig.align='center'}
# calculate distance matrix for max distances
distanceMatrix <- dist(sub1[,10:12])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering,lab.col=unclass(sub1$activity))
```


**Singular Value Decomposition**

```{r fig.height=4,fig.width=7,fig.align='center', cache=TRUE}
# perform SVD minus last two columns (subject and activity)
svd1 = svd(scale(sub1[,-c(562,563)]))
# create 1 x 2 panel plot
par(mfrow=c(1,2))
# plot first two left singular vector
# separate moving from non moving
plot(svd1$u[,1],col=sub1$activity,pch=19, main = "First Left Singular Vector")
plot(svd1$u[,2],col=sub1$activity,pch=19, main = "Second Left Singular Vector")
```


**New Clustering with Maximum Contributers**

```{r fig.height=5,fig.width=7,fig.align='center'}
# find the max contributing feature
maxContrib <- which.max(svd1$v[,2])
# recalculate distance matrix
distanceMatrix <- dist(sub1[, c(10:12,maxContrib)])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering,lab.col=unclass(sub1$activity))
# name of max contributing factor
names(samsungData)[maxContrib]
```


**K-means Clustering (nstart=1, first try)**

```{r}
# specify 6 centers for data
kClust <- kmeans(sub1[,-c(562,563)],centers=6)
# tabulate 6 clusteres against 6 activity but many clusters contain multiple activities
table(kClust$cluster,sub1$activity)
```

**K-means clustering (nstart=100, first try)**

```{r cache=TRUE}
# run k-means algorithm 100 times
kClust <- kmeans(sub1[,-c(562,563)],centers=6,nstart=100)
# tabulate results
table(kClust$cluster,sub1$activity)
```

**K-means clustering (nstart=100, second try)**

```{r cache=TRUE}
# run k-means algorithm 100 times
kClust <- kmeans(sub1[,-c(562,563)],centers=6,nstart=100)
# tabulate results
table(kClust$cluster,sub1$activity)
```

**Cluster 1 Variable Centers (Laying)**

```{r fig.height=4,fig.width=5, fig.align='center'}
# plot first 10 centers of k-means for laying to understand which features drive the activity
plot(kClust$center[1,1:10],pch=19,ylab="Cluster Center",xlab="")
```


**Cluster 2 Variable Centers (Walking)**

```{r fig.height=4,fig.width=5, fig.align='center'}
# plot first 10 centers of k-means for laying to understand which features drive the activity
plot(kClust$center[4,1:10],pch=19,ylab="Cluster Center",xlab="")
```


$\pagebreak$

## Case Study: Fine Particle Pollution in the U.S. from 1999 to 2012

**Read Raw Data from 1999 and 2012**

```{r cache = TRUE}
# read in raw data from 1999
pm0 <- read.table("pm25_data/RD_501_88101_1999-0.txt", comment.char = "#", header = FALSE, sep = "|", na.strings = "")
# read in headers/column lables
cnames <- readLines("pm25_data/RD_501_88101_1999-0.txt", 1)
# convert string into vector
cnames <- strsplit(substring(cnames, 3), "|", fixed = TRUE)
# make vector the column names
names(pm0) <- make.names(cnames[[1]])
# we are interested in the pm2.5 readings in the "Sample.Value" column
x0 <- pm0$Sample.Value
# read in the data from 2012
pm1 <- read.table("pm25_data/RD_501_88101_2012-0.txt", comment.char = "#", header = FALSE, sep = "|",
	na.strings = "", nrow = 1304290)
# make vector the column names
names(pm1) <- make.names(cnames[[1]])
# take the 2012 data for pm2.5 readings
x1 <- pm1$Sample.Value
```

**Summaries for Both Periods**

```{r}
# generate 6 number summaries
summary(x1)
summary(x0)
# calculate % of missing values, Are missing values important here?
data.frame(NA.1990 = mean(is.na(x0)), NA.2012 = mean(is.na(x1)))
```

**Make a boxplot of both 1999 and 2012**

```{r warning=FALSE, fig.width = 7, fig.height = 4, fig.align = 'center'}
par(mfrow = c(1,2))
# regular boxplot, data too right skewed
boxplot(x0, x1, main = "Regular Boxplot")
# log boxplot, significant difference in means, but more spread
boxplot(log10(x0), log10(x1), main = "log Boxplot")
```

**Check for Negative Values in 'x1'**

```{r fig.width = 5, fig.height = 4, fig.align = 'center'}
# summary again
summary(x1)
# create logical vector for
negative <- x1 < 0
# count number of negatives
sum(negative, na.rm = T)
# calculate percentage of negatives
mean(negative, na.rm = T)
# capture the date data
dates <- pm1$Date
dates <- as.Date(as.character(dates), "%Y%m%d")
# plot the histogram
hist(dates, "month")  ## Check what's going on in months 1--6
```

**Check Same New York Monitors at 1999 and 2012**

```{r}
# find unique monitors in New York in 1999
site0 <- unique(subset(pm0, State.Code == 36, c(County.Code, Site.ID)))
# find unique monitors in New York in 2012
site1 <- unique(subset(pm1, State.Code == 36, c(County.Code, Site.ID)))
# combine country codes and siteIDs of the monitors
site0 <- paste(site0[,1], site0[,2], sep = ".")
site1 <- paste(site1[,1], site1[,2], sep = ".")
# find common monitors in both
both <- intersect(site0, site1)
# print common monitors in 1999 and 2012
print(both)
```

**Find how many observations available at each monitor**

```{r}
# add columns for combined county/site for the original data
pm0$county.site <- with(pm0, paste(County.Code, Site.ID, sep = "."))
pm1$county.site <- with(pm1, paste(County.Code, Site.ID, sep = "."))
# find subsets where state = NY and county/site = what we found previously
cnt0 <- subset(pm0, State.Code == 36 & county.site %in% both)
cnt1 <- subset(pm1, State.Code == 36 & county.site %in% both)
# split data by the county/size values and count oberservations
sapply(split(cnt0, cnt0$county.site), nrow)
sapply(split(cnt1, cnt1$county.site), nrow)
```

**Choose Monitor where County = 63 and Side ID = 2008**

```{r}
# filter data by state/county/siteID
pm1sub <- subset(pm1, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
pm0sub <- subset(pm0, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
# there are 30 observations from 2012, and 122 from 1999
dim(pm1sub)
dim(pm0sub)
```

**Plot Data for 2012**

```{r fig.width=4, fig.height=4, fig.align = 'center'}
# capture the dates of the subset of data
dates1 <- pm1sub$Date
# capture measurements for the subset of data
x1sub <- pm1sub$Sample.Value
# convert dates to appropriate format
dates1 <- as.Date(as.character(dates1), "%Y%m%d")
# plot pm2.5 value vs time
plot(dates1, x1sub, main = "PM2.5 Polution Level in 2012")
```

**Plot data for 1999**

```{r fig.width=4, fig.height=4, fig.align = 'center'}
# capture the dates of the subset of data
dates0 <- pm0sub$Date
# convert dates to appropriate format
dates0 <- as.Date(as.character(dates0), "%Y%m%d")
# capture measurements for the subset of data
x0sub <- pm0sub$Sample.Value
# plot pm2.5 value vs time
plot(dates0, x0sub, main = "PM2.5 Polution Level in 1999")
```

**Panel Plot for Both Years**

```{r fig.width=6, fig.height=4, fig.align = 'center'}
# find max range for data
rng <- range(x0sub, x1sub, na.rm = T)
# create 1 x 2 panel plot
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
# plot time series plot for 1999
plot(dates0, x0sub, pch = 20, ylim = rng, main="Pollution in 1999")
# plot the median
abline(h = median(x0sub, na.rm = T))
# plot time series plot for 2012
plot(dates1, x1sub, pch = 20, ylim = rng, main="Pollution in 2012")
# plot the median
abline(h = median(x1sub, na.rm = T))
```

**Find State-wide Means and Trend**

```{r fig.width=5, fig.height=5, fig.align = 'center'}
# divide data by state and find tne mean of pollution level for 1999
mn0 <- with(pm0, tapply(Sample.Value, State.Code, mean, na.rm = T))
# divide data by state and find tne mean of pollution level for 1999
mn1 <- with(pm1, tapply(Sample.Value, State.Code, mean, na.rm = T))
# convert to data frames while preserving state names
d0 <- data.frame(state = names(mn0), mean = mn0)
d1 <- data.frame(state = names(mn1), mean = mn1)
# merge the 1999 and 2012 means by state
mrg <- merge(d0, d1, by = "state")
# dimension of combined data frame
dim(mrg)
# first few lines of data
head(mrg)

# plot the pollution levels data points for 1999
with(mrg, plot(rep(1, 52), mrg[, 2], xlim = c(.8, 2.2), ylim = c(3, 20),
	main = "PM2.5 Pollution Level by State for 1999 & 2012",
	xlab = "", ylab = "State-wide Mean PM"))
# plot the pollution levels data points for 2012
with(mrg, points(rep(2, 52), mrg[, 3]))
# connected the dots
segments(rep(1, 52), mrg[, 2], rep(2, 52), mrg[, 3])
# add 1999 and 2012 labels
axis(1, c(1, 2), c("1999", "2012"))
```
